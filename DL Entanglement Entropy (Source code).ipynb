{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff10ae1",
   "metadata": {},
   "source": [
    "### The following provides the source code for reproducing results in section 3 of arXiv: 2305.00997 of using classical deep neural networks to predict von Neumann entropy. This is based on TensorFlow-Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python import metrics\n",
    "import keras_tuner as kt\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science','no-latex'])\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "\n",
    "\n",
    "\"Hyperparameters\" \n",
    "# Please adjust the hyperparameters and the hypermodel_base function below.\n",
    "\n",
    "# the following are hyperparameters for the KerasTuner\n",
    "max_trials = 100\n",
    "executions_per_trial = 2\n",
    "patience = 8\n",
    "search_epochs = 500\n",
    "\n",
    "top_n = 5 # we pick the top_n models, note that this has to be at least the same as the max-trials\n",
    "\n",
    "# the following are hyperparameters for retrained models\n",
    "batch_size = 512\n",
    "Retrain_training_times = 20 \n",
    "retrain_patience = 8\n",
    "retrain_max_epochs = 1000\n",
    "\n",
    "\n",
    "def hypermodel_base(hp):\n",
    "    #units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
    "    activation = hp.Fixed(\"activation\", \"relu\")\n",
    "    #learning_rate = hp.Fixed(\"learning_rate\", 9e-3)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=3e-3, max_value=9e-3, sampling=\"log\")\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.1, max_value=0.5, sampling=\"log\")  # step=0.1\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=42) # GlorotNormal is the default one, we add a seed\n",
    "\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 4)):\n",
    "        if hp.Boolean(\"BatchNormalization\"):  \n",
    "            model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(f\"units_{i}\", min_value=16, max_value=128, step=16), # num of units will be indep\n",
    "                use_bias = False, # if we use BatchNormalization, set bias to false and no activation\n",
    "                kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation(activation=activation)) # set activation after BatchNormalization\n",
    "        else:\n",
    "            model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(f\"units_{i}\", min_value=16, max_value=128, step=16), # num of units will be indep\n",
    "                activation=activation,\n",
    "                kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07,\n",
    "            amsgrad=True)  #hp.Boolean(\"amsgrad\")\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=\"mse\",\n",
    "              metrics=[\"mae\"])\n",
    "    return model \n",
    "\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, path, ratio=(0.8,0.1,0.1)): \n",
    "        X, y = self.load_dataset(path)\n",
    "        self.split_dataset(X, y, ratio)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def load_dataset(self, path): \n",
    "        df = pd.read_csv(path, encoding='utf-8')\n",
    "        X = df.drop(['Correct Entropy','Approx Entropy'], axis = 1)\n",
    "        y = df['Correct Entropy']\n",
    "        return X, y\n",
    "\n",
    "    def shuffle_dataset(self, path):\n",
    "        df = pd.read_csv(path, encoding='utf-8')\n",
    "        df_shuffle = df.sample(frac=1., axis=0, random_state = 42).reset_index(drop=True)\n",
    "        df_shuffle1 = df_shuffle.drop(['Correct Entropy', 'Approx Entropy'], axis = 1)\n",
    "        y_targets = df_shuffle['Correct Entropy']\n",
    "        return df_shuffle1, y_targets\n",
    "\n",
    "    def split_dataset(self, X, y, ratio):\n",
    "        '''\n",
    "        Args:\n",
    "            X: the training inputs array\n",
    "            y: the ground truth data array\n",
    "            ratio: (train, validation, test)\n",
    "        '''\n",
    "        assert sum(ratio) == 1\n",
    "        seed = 42\n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=ratio[2], random_state = seed)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, \n",
    "                                                          test_size=round(ratio[1]/(ratio[0]+ratio[1]), 16), \n",
    "                                                          random_state=seed) \n",
    "\n",
    "        self.X_train_full = X_train_full\n",
    "        self.y_train_full = y_train_full\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "\n",
    "def load_tuner(hypermodel, load_dir, load_project):\n",
    "    tuner = kt.BayesianOptimization(hypermodel,\n",
    "                    objective=\"val_loss\", \n",
    "                    directory=load_dir, \n",
    "                    project_name=load_project,\n",
    "                    overwrite=False,\n",
    "                    max_trials=max_trials, \n",
    "                    executions_per_trial=executions_per_trial, \n",
    "                    )\n",
    "    return tuner\n",
    "\n",
    "\n",
    "def load_best_hyperparam(hypermodel, load_dir, load_project, top_n):\n",
    "    tuner = load_tuner(hypermodel, load_dir, load_project)\n",
    "    return tuner.get_best_hyperparameters(top_n)\n",
    "\n",
    "  # if you want to see the top_n hyper params, use below\n",
    "  # best_trials = tuner.oracle.get_best_trials(num_trials=top_n)\n",
    "  # for trial in best_trials:\n",
    "    # trial.summary()\n",
    "    # model = tuner.load_model(trial)\n",
    "    # Do some stuff to the model\n",
    "\n",
    "\n",
    "\n",
    "def train_with_tuner_ensemble_models(data_path, hypermodel, hp_dir, hp_proj): \n",
    "    # the saved checkpoint is under \"hp_dir/hp_proj/\"\n",
    "\n",
    "    dataloader = DataLoader(data_path)\n",
    "    tuner = load_tuner(hypermodel, hp_dir, hp_proj)\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience)]\n",
    "    #  keras.callbacks.ModelCheckpoint(model_name, save_best_only=True)] \n",
    "    tuner.search(x = dataloader.X_train, y = dataloader.y_train, \n",
    "                batch_size = batch_size, epochs=search_epochs, validation_data=(dataloader.X_val, dataloader.y_val), \n",
    "                callbacks=callbacks, verbose=1,)\n",
    "    tuner.results_summary()\n",
    "    return tuner\n",
    "\n",
    "\n",
    "def train_with_tuner_ensemble_models_retrain(path_train, path_val, hypermodel, best_hps, max_epochs, model_save_path): \n",
    "    # Now we re-train each model with the best epoch*ratio and the full data.\n",
    "    # We monitor loss with EarlyStopping, the epoch*ratio will be the maximum possible epoch.\n",
    "    # We train each model multiple times and use the one with the best training loss.\n",
    "    \n",
    "    dataloader = DataLoader(path_train)\n",
    "    dataloader_val = DataLoader(path_val)\n",
    "    X_full, y_full = dataloader_val.shuffle_dataset(path_val)\n",
    "    ratio = (len(dataloader.X_train)+len(dataloader.X_val))/len(dataloader.X_train)\n",
    "    \n",
    "    # best_hps = tuner.get_best_hyperparameters(top_n)\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"loss\", patience=retrain_patience, restore_best_weights=True)]\n",
    "    \n",
    "    model_retrain = {}\n",
    "    model_history_retrain = {}\n",
    "    \n",
    "    df_store_test = {}\n",
    "    df_store_test_final = {}\n",
    "    df_store_val = {}\n",
    "    df_store_test_over_training = {}\n",
    "    df_store_val_over_training = {}\n",
    "    \n",
    "    relative_errors_sum = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_df = pd.DataFrame(dataloader.y_test).reset_index(drop=True) # the targets of the original train_test split\n",
    "    pred_val_df = pd.DataFrame(y_full) # the targets of the full validation dataset\n",
    "    \n",
    "    Average_Epoch_List = []\n",
    "    \n",
    "    model_re_save = {}\n",
    "    model_number_save = []\n",
    "    \n",
    "    for i in range(0, top_n):   \n",
    "#         n = 0\n",
    "#         while n < training_times:\n",
    "#             Average_Epoch_List.append(max_epochs)\n",
    "#             Average_Epoch_List.append(Average_Epoch[str(i) + str(n)])\n",
    "#             n = n + 1\n",
    "        j = 0\n",
    "        while j < Retrain_training_times:\n",
    "#             best_epoch = int(np.array(Average_Epoch_List).mean())\n",
    "#             Best_training_epochs = int(best_epoch * ratio)\n",
    "            model_retrain[str(i)] = hypermodel(best_hps[i])\n",
    "            model_re = model_retrain[str(i)]\n",
    "\n",
    "            model_re.fit(dataloader.X_train_full, dataloader.y_train_full, \n",
    "                          epochs = max_epochs,\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks=callbacks,\n",
    "                          verbose=1,\n",
    "                  )\n",
    "            \n",
    "            model_re_save[str(i) + str(j)] = model_re\n",
    "            model_history_retrain[str(i) + str(j)] = model_re.history.history\n",
    "\n",
    "            test_predictions = model_re.predict(dataloader.X_test) # for the original train-test split\n",
    "            test_pred = pd.DataFrame(test_predictions)\n",
    "            test_pred.columns = ['Model_' + str(i) + ' #'+ str(j) + ' Predictions']\n",
    "            df_store_test[str(i) + str(j)] = test_pred\n",
    "            \n",
    "            # save final model\n",
    "            model_re.save(os.path.join(model_save_path, f'top_{i}_times_{j}'))\n",
    "\n",
    "            j = j + 1\n",
    "\n",
    "\n",
    "        # We choose the smallest overall relative error in the \"test dataset\" as the final output\n",
    "        df_test_all = df_store_test[str(i) + str(0)]\n",
    "        for k in range(1, Retrain_training_times):\n",
    "            df_test_all = pd.concat([df_test_all, df_store_test[str(i) + str(k)]], axis=1)\n",
    "            \n",
    "        df_compare = pd.concat([pred_df, df_test_all], axis = 1)\n",
    "        for l in range(0, Retrain_training_times):\n",
    "            df_rel_error = (abs(df_compare['Correct Entropy']-df_compare['Model_' + str(i) + ' #'+ str(l) + ' Predictions'])/df_compare['Correct Entropy'])*100\n",
    "            df_compare.insert(2+2*l, str(i)+'Rel Error (%) ' + str(l), df_rel_error)\n",
    "        print(df_compare)\n",
    "        relative_errors_sum[str(i)] = df_compare[[str(i)+'Rel Error (%) '+str(m) for m in range(0, Retrain_training_times)]].sum()\n",
    "        model_number = df_compare[[str(i)+'Rel Error (%) '+str(m) for m in range(0, Retrain_training_times)]].sum().argmin()\n",
    "        model_number_save.append(model_number)\n",
    "        print(f\"{relative_errors_sum[str(i)]}\")\n",
    "        print(f\"# {model_number} has the smallest relative errors.\")\n",
    "        df_store_test_final[str(i)] = df_compare['Model_' + str(i) + ' #'+ str(model_number) + ' Predictions']\n",
    "        \n",
    "        # Then we choose the final model to predict the PredVal dataset\n",
    "        model_re = model_re_save[str(i)+str(model_number)]\n",
    "        \n",
    "        val_predictions = model_re.predict(X_full) # for the Pred_validation dataset\n",
    "        val_pred = pd.DataFrame(val_predictions)\n",
    "        val_pred.columns = ['Model_' + str(i) + ' Pred_Val']\n",
    "        df_store_val[str(i)] = val_pred\n",
    "\n",
    "\n",
    "\n",
    "        # save final model\n",
    "        model_re.save(os.path.join(model_save_path, 'top_'+str(i)))\n",
    "    \n",
    "    relative_errors_sum_df = pd.DataFrame(relative_errors_sum[str(0)])\n",
    "    for k in range(0, top_n):\n",
    "        relative_errors_sum_df = pd.concat([relative_errors_sum_df, pd.DataFrame(relative_errors_sum[str(k)])], axis=0)\n",
    "    print(f\"# {relative_errors_sum_df.idxmin()} has the smallest relative errors.\")\n",
    "    filepath = os.path.join(model_save_path, 'model_history_retrain.json') \n",
    "    with open(filepath, 'w') as handle:\n",
    "        json.dump(model_history_retrain, handle)\n",
    "        \n",
    "\n",
    "    model_numbers_save_str = [str(i) for i in model_number_save]\n",
    "    path = os.path.join(*model_numbers_save_str)\n",
    "    with open(os.path.join(model_save_path, \"model_number.txt\"), \"w\") as f:\n",
    "        f.write(path)\n",
    "    \n",
    "     # this shows how to load a model\n",
    "#     print(load_model(save_model_path+'/top_0').summary())\n",
    "#     print(load_model(save_model_path+'/top_1').summary())\n",
    "\n",
    "        \n",
    "    return df_store_test_final, df_store_val, pred_df, pred_val_df, df_compare\n",
    "\n",
    "\n",
    "def Ensemble_Retrained_Plots(save_model_path, save_path, best_model):\n",
    "    \n",
    "    # Load the model_history_retrain\n",
    "    with open(os.path.join(save_model_path, \"model_history_retrain.json\"), \"r\") as f:\n",
    "        model_history_retrain = json.load(f)\n",
    "\n",
    "    # Load the model_number\n",
    "    with open(os.path.join(save_model_path, \"model_number.txt\"), \"r\") as f:\n",
    "        contents = f.read()\n",
    "    contents = contents.split(\"\\n\")\n",
    "    contents = [i.replace(\"\\\\\", \"\") for i in contents]\n",
    "    str_list = str(contents[0])\n",
    "    model_number_save = [int(i) for i in str_list]\n",
    "        \n",
    "    x_range_list = []    \n",
    "    legend_labels = []\n",
    "    #fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=False, figsize=(20, 16))\n",
    "    fig, (ax1, ax3) = plt.subplots(2, 1, sharex=False, figsize=(20, 16))\n",
    "    y_max = []\n",
    "    y_max_mae = []\n",
    "    j = 0\n",
    "    while j < top_n:\n",
    "        y_max.append(max(model_history_retrain[str(j)+str(model_number_save[j])][\"loss\"][:].copy()))\n",
    "        y_max_mae.append(max(model_history_retrain[str(j)+str(model_number_save[j])][\"mae\"][:].copy()))\n",
    "        j = j +1\n",
    "\n",
    "    i = 0\n",
    "    while i < top_n:\n",
    "        y_loss = model_history_retrain[str(i)+str(model_number_save[i])]['loss']\n",
    "        y_metrics = model_history_retrain[str(i)+str(model_number_save[i])][\"mae\"]\n",
    "        x = range(1,len(y_loss)+1)\n",
    "\n",
    "        ax1.plot(x, y_loss, '.--')\n",
    "        #ax2.plot(x, y_loss, '.--')\n",
    "\n",
    "        ax3.plot(x, y_metrics, '.--')\n",
    "        #ax4.plot(x, y_metrics, '.--')\n",
    "\n",
    "        x_range_list.append(len(y_loss))\n",
    "        \n",
    "        legend_labels.append('Model_' + str(i) + ' (Epoch= '+ str(x_range_list[i])+ ')')\n",
    "        \n",
    "        i = i + 1\n",
    "    \n",
    "    y_loss = model_history_retrain[str(i)+str(model_number_save[i])]['loss']\n",
    "    y_metrics = model_history_retrain[str(i)+str(model_number_save[i])][\"mae\"]\n",
    "    x = range(1,len(y_loss)+1)\n",
    "\n",
    "    ax1.plot(x, y_loss, '.--')\n",
    "    #ax2.plot(x, y_loss, '.--')\n",
    "\n",
    "    ax3.plot(x, y_metrics, '.--')\n",
    "    #ax4.plot(x, y_metrics, '.--')\n",
    "\n",
    "    x_range_list.append(len(y_loss))\n",
    "\n",
    "    legend_labels.append('Model_' + str(i) + ' (Epoch= '+ str(x_range_list[i])+ ')')\n",
    "        \n",
    "\n",
    "    fig.legend(legend_labels, loc=\"right\", bbox_to_anchor=(1.13, 0.5), frameon=False, edgecolor='black', borderpad=1, labelspacing=2, handlelength=3) \n",
    "    ax1.set_title(\"Loss Function\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"MSE\")\n",
    "    ax1.set_xlim([1, len(y_loss)-retrain_patience])\n",
    "    ax1.set_xticks(range(50,max(x_range_list),50))\n",
    "    ax1.set_yscale('log')\n",
    "    #ax1.set_xticklabels(range(5,max(x_range_list),5))\n",
    "\n",
    "\n",
    "    ax3.set_title(\"Metrics\")\n",
    "    ax3.set_xlabel(\"Epochs\")\n",
    "    ax3.set_ylabel(\"MAE\")\n",
    "    ax3.set_xlim([1, len(y_loss)-retrain_patience])\n",
    "    ax3.set_xticks(range(50,max(x_range_list),50))\n",
    "    #ax3.set_xticklabels(range(5,max(x_range_list),5))\n",
    "    ax3.set_yscale('log')\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'Ensem_Retrain.png'))\n",
    "    \n",
    "def generate_figures(train_data_path, test_data_path, model_path, save_fig_dir, best_model, best_model_number):\n",
    "    \n",
    "    i = best_model\n",
    "    j = best_model_number\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data_path)\n",
    "    df_train = pd.DataFrame(train_dataloader.y_test)\n",
    "    df_train\n",
    "    df = pd.read_csv(test_data_path, encoding='utf-8')\n",
    "    df_shuffle = df.sample(frac=1., axis=0, random_state = 42)\n",
    "    X_test  = df_shuffle.drop(['Correct Entropy', 'Approx Entropy'], axis = 1)\n",
    "    y_test = df_shuffle['Correct Entropy']\n",
    "    df_test = pd.DataFrame(y_test)\n",
    "    df_test\n",
    "    \n",
    "    ymin = pd.concat([df_train, df_test], axis = 1)[['Correct Entropy','Correct Entropy']].min().min()*0.90\n",
    "    ymax = pd.concat([df_train, df_test], axis = 1)[['Correct Entropy','Correct Entropy']].max().max()*1.05\n",
    "\n",
    "    def compute_model_prediction(X_test, y_test, model):\n",
    "        inputs = X_test.to_numpy()\n",
    "        actual = y_test.to_numpy()\n",
    "        approx = inputs.sum(1)\n",
    "        sort_idx = np.argsort(actual)\n",
    "        inputs, actual, approx = inputs[sort_idx], actual[sort_idx], approx[sort_idx]\n",
    "        pred = model.predict(inputs).reshape(-1)\n",
    "        \n",
    "        return (pred, actual, approx)\n",
    "\n",
    "\n",
    "#     def plot_entropy_comparison(pred, actual, approx, save_fig_dir):\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         plt.plot(actual)\n",
    "#         plt.plot(pred)\n",
    "#         plt.plot(approx)\n",
    "#         plt.legend([\"von Neumann entropy\", \"Model predictions\", \"Approximate entropy\"])\n",
    "#         plt.ylim([ymin, ymax])\n",
    "#         plt.savefig(save_fig_dir)\n",
    "#         plt.show()\n",
    "\n",
    "    def plot_entropy_comparison1(pred, actual, approx, save_fig_dir):\n",
    "        # Set the limits for the zoomed-in region\n",
    "        zoom_xlim = (600, 700)\n",
    "        zoom_ylim = (1.91, 1.98)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.plot(actual)\n",
    "        ax.plot(pred)\n",
    "        ax.plot(approx)\n",
    "        ax.legend([\"von Neumann entropy\", \"Model predictions\", \"Approximate entropy\"])\n",
    "        ax.set_ylim([ymin, ymax])\n",
    "        \n",
    "        # Create zoomed-in inset axes\n",
    "        axins = zoomed_inset_axes(ax, zoom=7, loc='lower right', bbox_to_anchor=(0.6,0.05,.3,.3), bbox_transform=ax.transAxes)\n",
    "        \n",
    "        axins.plot(actual)\n",
    "        axins.plot(pred)\n",
    "        axins.plot(approx)\n",
    "\n",
    "        # Set the limits for the zoomed-in region\n",
    "        axins.set_xlim(*zoom_xlim)\n",
    "        axins.set_ylim(*zoom_ylim)\n",
    "\n",
    "        # Mark the region of the inset in the main plot\n",
    "        mark_inset(ax, axins, loc1=1, loc2=2, fc=\"none\", ec=\"0.5\")\n",
    "        \n",
    "        \n",
    "        plt.savefig(save_fig_dir)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def plot_entropy_comparison2(pred, actual, approx, save_fig_dir):\n",
    "        # Set the limits for the zoomed-in region\n",
    "        zoom_xlim = (6000, 7000)\n",
    "        zoom_ylim = (2.22, 2.26)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.plot(actual)\n",
    "        ax.plot(pred)\n",
    "        ax.plot(approx)\n",
    "        ax.legend([\"von Neumann entropy\", \"Model predictions\", \"Approximate entropy\"])\n",
    "        ax.set_ylim([ymin, ymax])\n",
    "        \n",
    "        # Create zoomed-in inset axes\n",
    "        axins = zoomed_inset_axes(ax, zoom=7, loc='lower right', bbox_to_anchor=(0.6,0.1,.3,.3), bbox_transform=ax.transAxes)\n",
    "        \n",
    "        \n",
    "        axins.plot(actual)\n",
    "        axins.plot(pred)\n",
    "        axins.plot(approx)\n",
    "\n",
    "        # Set the limits for the zoomed-in region\n",
    "        axins.set_xlim(*zoom_xlim)\n",
    "        axins.set_ylim(*zoom_ylim)\n",
    "\n",
    "        # Mark the region of the inset in the main plot\n",
    "        mark_inset(ax, axins, loc1=1, loc2=2, fc=\"none\", ec=\"0.5\")\n",
    "        \n",
    "\n",
    "        \n",
    "        plt.savefig(save_fig_dir)\n",
    "        plt.show()\n",
    "    \n",
    "    model = load_model(save_model_path+'/top_'+str(i)+'_times_'+str(j))\n",
    "    \n",
    "    # plot loss and metric\n",
    "    # Load the model_history_retrain\n",
    "    with open(os.path.join(save_model_path, \"model_history_retrain.json\"), \"r\") as f:\n",
    "        model_history_retrain = json.load(f)\n",
    "\n",
    "#     fig, (ax1, ax2) = plt.subplots(2, 1, sharex=False, figsize=(20, 16))\n",
    "    fig1, ax1 = plt.subplots(figsize=(15, 5))\n",
    "#     fig2, ax2 = plt.subplots(figsize=(15, 12))\n",
    "    y_loss = model_history_retrain[str(i)+str(j)]['loss']\n",
    "    y_metrics = model_history_retrain[str(i)+str(j)][\"mae\"]\n",
    "    x = range(1,len(y_loss)+1)\n",
    "\n",
    "    ax1.plot(x, y_loss, '--', alpha=1.0, color=(1,0,0))\n",
    "#     ax2.plot(x, y_metrics, '--', alpha=1.0, color=(1,0,0))\n",
    "    \n",
    "    ax1.set_title(\"Loss Function\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"MSE\")\n",
    "    ax1.set_xlim([1, len(y_loss)-retrain_patience])\n",
    "    xticks = list(range(50,len(y_loss)-retrain_patience,50))\n",
    "    xticks.append(len(y_loss)-retrain_patience)\n",
    "    ax1.set_xticks(xticks)\n",
    "    #ax1.set_xticks(range(50,len(y_loss)-retrain_patience,50))\n",
    "    ax1.set_yscale('log')\n",
    "    plt.savefig(os.path.join(save_fig_dir, 'Loss.jpg'))\n",
    "    \n",
    "#     ax2.set_title(\"Metric\")\n",
    "#     ax2.set_xlabel(\"Epochs\")\n",
    "#     ax2.set_ylabel(\"MAE\")\n",
    "#     ax2.set_xlim([1, len(y_loss)-retrain_patience])\n",
    "#     ax2.set_xticks(xticks)\n",
    "#     ax2.set_yscale('log')\n",
    "#     fig.tight_layout()\n",
    "#     plt.savefig(os.path.join(save_fig_dir, 'Ensem_Retrain.png'))\n",
    "    \n",
    "    # plot test dataset\n",
    "    train_dataloader = DataLoader(train_data_path)\n",
    "    X_test = train_dataloader.X_test\n",
    "    y_test = train_dataloader.y_test\n",
    "    train_pred = compute_model_prediction(X_test, y_test, model)\n",
    "    plot_entropy_comparison(*train_pred, save_fig_dir+\"/entropy_compare1.jpg\")\n",
    "\n",
    "    # plot unseen dataset\n",
    "    df = pd.read_csv(test_data_path, encoding='utf-8')\n",
    "    df_shuffle = df.sample(frac=1., axis=0, random_state = 42)\n",
    "    X_test  = df_shuffle.drop(['Correct Entropy', 'Approx Entropy'], axis = 1)\n",
    "    y_test = df_shuffle['Correct Entropy']\n",
    "    test_pred = compute_model_prediction(X_test, y_test, model)\n",
    "    plot_entropy_comparison(*test_pred, save_fig_dir+\"/entropy_compare2.jpg\")\n",
    "\n",
    "    # relative error\n",
    "    diff = train_pred[0]-train_pred[1]\n",
    "    train_error = (np.abs(diff)/train_pred[1])*100.\n",
    "    diff = test_pred[0]-test_pred[1]\n",
    "    test_error = (np.abs(diff)/test_pred[1])*100.\n",
    "    fig_error, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "    ax1.plot(train_error, '.')\n",
    "    ax2.plot(test_error, '.')\n",
    "    ax1.set_title('Relative Errors (%) for Test Data')\n",
    "    ax2.set_title(\"Relative Errors (%) for Unseen Data\")\n",
    "    plt.savefig(save_fig_dir+\"/relative_error.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "    # dist plot\n",
    "    fig_dist, ax3 = plt.subplots(1,1,figsize=(10,5))\n",
    "    sns.distplot(train_error)\n",
    "    sns.distplot(test_error)\n",
    "    ax3.set_title('Density Plot of Relative Errors for Test Data')\n",
    "    ax3.set_xlabel('Relative Errors (%)')\n",
    "    ax3.set_ylabel('Density')\n",
    "    plt.savefig(save_fig_dir+\"/relative_error_density.jpg\")\n",
    "\n",
    "def Ensembel_Tables_Generation(path, path_val, df_store_test_final, df_store_val, pred_df, pred_val_df, save_path):\n",
    "    \n",
    "    df_store_test_final\n",
    "\n",
    "    dataloader = DataLoader(path)\n",
    "\n",
    "    dataloader_val = DataLoader(path_val)\n",
    "    X_full, y_full = dataloader_val.shuffle_dataset(path_val)\n",
    "    \n",
    "    \n",
    "    df_test_all = df_store_test_final[str(0)]\n",
    "    df_val_all = df_store_val[str(0)]\n",
    "#     for i in range(1, top_n):\n",
    "#         df_test_all = pd.concat([df_test_all, df_store_test_final[str(i)]], axis=1)\n",
    "#         df_val_all = pd.concat([df_val_all, df_store_val[str(i)]], axis=1)\n",
    "\n",
    "#     df_test_all_mean = df_test_all.mean(axis=1)\n",
    "\n",
    "#     df_compare = pd.concat([pred_df, df_test_all_mean, df_test_all], axis = 1)\n",
    "\n",
    "    df_compare = pd.concat([pred_df, df_test_all], axis = 1)\n",
    "    # Renaming the columns\n",
    "    old_column_names = df_compare.columns.tolist()\n",
    "    column_mapping = {old_column_names[0]: 'Correct Entropy', old_column_names[1]: 'Model Predictions'}\n",
    "    df_compare = df_compare.rename(columns=column_mapping)\n",
    "    \n",
    "    df_rel_error = (abs(df_compare['Correct Entropy']-df_compare['Model Predictions'])/df_compare['Correct Entropy'])*100\n",
    "    df_compare.insert(2, 'Relative Errors (%) for Models', df_rel_error)\n",
    "    df_compare = df_compare.sort_values(by=['Correct Entropy']) # reorder the values\n",
    "    print(df_compare)\n",
    "\n",
    "#    df_val_all_mean = df_val_all.mean(axis=1)\n",
    "\n",
    "#    df_compare_val = pd.concat([pred_val_df, df_val_all_mean, df_val_all], axis = 1)\n",
    "\n",
    "    df_compare_val = pd.concat([pred_val_df, df_val_all], axis = 1)\n",
    "    \n",
    "    # Renaming the columns\n",
    "    old_column_names = df_compare_val.columns.tolist()\n",
    "    column_mapping = {old_column_names[0]: 'Correct Entropy', old_column_names[1]: 'Model Predictions'}\n",
    "    df_compare_val = df_compare_val.rename(columns=column_mapping)\n",
    "    \n",
    "    df_rel_error_val = (abs(df_compare_val['Correct Entropy']-df_compare_val['Model Predictions'])/df_compare_val['Correct Entropy'])*100\n",
    "    df_compare_val.insert(2, 'Relative Errors (%) for Models', df_rel_error_val)\n",
    "    df_compare_val = df_compare_val.sort_values(by=['Correct Entropy']) # reorder the values\n",
    "    print(df_compare_val)\n",
    "\n",
    "    fig_error, (ax5, ax6) = plt.subplots(1, 2, figsize=(10,5))\n",
    "    ax5.plot(np.arange(0, len(dataloader.X_test)), df_compare['Relative Errors (%) for Models'], '.')\n",
    "    ax5.set_title(\"Relative Errors (%) for Test Data\")\n",
    "\n",
    "    ax6.plot(np.arange(0, len(X_full)), df_compare_val['Relative Errors (%) for Models'], '.')\n",
    "    ax6.set_title(\"Relative Errors (%) for Unseen Data\")\n",
    "\n",
    "    fig_error.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'Ensemble_Table_Gen.png'))\n",
    "    return df_compare, df_compare_val\n",
    "\n",
    "def Test_Set_Plot(path, df_compare, df_compare_val, save_path):\n",
    "    # The plot will automatically determine the lower and upper ylim based on the smallest and largest values of the Test and PredVal Sets.\n",
    "    # We need to input both df_compare and df_compare_val\n",
    "    df = pd.read_csv(path, encoding='utf-8')\n",
    "    X = df.drop(['Correct Entropy'], axis = 1)\n",
    "    y = df['Correct Entropy']\n",
    "    seed = 42\n",
    "    ratio=(0.8,0.1,0.1)\n",
    "    assert sum(ratio) == 1\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=ratio[2], random_state = seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, \n",
    "                                                          test_size=round(ratio[1]/(ratio[0]+ratio[1]), 16), \n",
    "                                                          random_state=seed) \n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    df_compare_1 = pd.concat([df_compare, X_test], axis = 1)\n",
    "    df_compare_2 = df_compare_1.sort_values(by='Correct Entropy', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # Determine the ylim\n",
    "    ylim=(pd.concat([df_compare, df_compare_val], axis = 1)[['Correct Entropy','Correct Entropy']].min().min()*0.95, pd.concat([df_compare, df_compare_val], axis = 1)[['Correct Entropy','Correct Entropy']].max().max()*1.05) \n",
    "    # We expand slightly the ylim.\n",
    "    df_compare_2.plot(y=['Correct Entropy', 'Model Predictions', 'Approx Entropy'], use_index=True, figsize=(10, 10), ylim=ylim)\n",
    "    plt.legend([\"von Neumann Entropy\", \"Model Predictions\", \"Approximate Entropy\"])\n",
    "    plt.savefig(os.path.join(save_path, 'Test_Set_Plot.png'))\n",
    "\n",
    "    return df_compare_2, ylim\n",
    "    \n",
    "\n",
    "def Pred_Val_Plot(path, df_compare_val, ylim, save_path):\n",
    "    # The plot will have the same ylim as the Test_Set_Plot\n",
    "    df = pd.read_csv(path, encoding='utf-8')\n",
    "    df_shuffle = df.sample(frac=1., axis=0, random_state = 42).reset_index(drop=True)\n",
    "    X = df_shuffle.drop(['Correct Entropy'], axis = 1)\n",
    "    \n",
    "    df_compare_1 = pd.concat([df_compare_val, X], axis = 1)\n",
    "    df_compare_2 = df_compare_1.sort_values(by='Correct Entropy', ascending=True).reset_index(drop=True)\n",
    "    df_compare_2.plot(y=['Correct Entropy', 'Model Predictions', 'Approx Entropy'], use_index=True, figsize=(10, 10), ylim=ylim)\n",
    "    plt.legend([\"von Neumann Entropy\", \"Model Predictions\", \"Approximate Entropy\"])\n",
    "    plt.savefig(os.path.join(save_path, 'Pred_Val_Plot.png'))\n",
    "\n",
    "    return df_compare_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6592d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put the directories of the datasets and the save paths below\n",
    "\n",
    "train_data_path = ''\n",
    "test_data_path = ''\n",
    "save_hp_dir = ''\n",
    "save_hp_proj = ''\n",
    "save_model_path = ''\n",
    "save_fig_dir = ''\n",
    "\n",
    "# this is to assure the path name is correct\n",
    "import os\n",
    "assert(os.path.exists(train_data_path))\n",
    "assert(os.path.exists(test_data_path))\n",
    "assert(os.path.exists(save_hp_dir))\n",
    "assert(os.path.exists(save_model_path))\n",
    "assert(os.path.exists(save_fig_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b446988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One could run the following line by line for each example considered in section 3 of arXiv:2305.00997\n",
    "\n",
    "# The following initialize the KerasTuner with the specified hyperparameters range\n",
    "tuner = train_with_tuner_ensemble_models(train_data_path, hypermodel_base, save_hp_dir, save_hp_proj)\n",
    "\n",
    "# The following load the best configurations found by KerasTuner and retrain the models by also including the validation data\n",
    "best_hps = load_best_hyperparam(hypermodel_base, save_hp_dir, save_hp_proj, top_n)\n",
    "df_store_test_final, df_store_val, pred_df, pred_val_df, df_compare = train_with_tuner_ensemble_models_retrain(train_data_path, test_data_path, hypermodel_base, best_hps, retrain_max_epochs, save_model_path)\n",
    "\n",
    "# The following generates the figures for the loss function, predictions, and the relative errors.\n",
    "generate_figures(train_data_path, test_data_path, save_model_path, save_fig_dir, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dcff10",
   "metadata": {},
   "source": [
    "### The following provides the source code for reproducing results in section 4 of arXiv: 2305.00997 of using treating the Renyi entropies as sequential deep learning. Again based on TensorFlow-Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6bd48fba",
   "metadata": {
    "id": "6bd48fba"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import scienceplots\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plt.style.reload_library()\n",
    "# plt.style.use(['science','no-latex'])\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "\n",
    "\"Hyperparameters\" \n",
    "# Please adjust the hyperparameters and the parameters in hyper_RNN_model below.\n",
    "\n",
    "# The following are hyperparameters for the datasets\n",
    "MIN = 5 # the length of each vector\n",
    "k = 1000 # how many datasets we want to use out of the full 10000 sets\n",
    "ratio_original = (0.6, 0.2, 0.2)\n",
    "ratio = (0.8, 0.0, 0.2) \n",
    "\n",
    "# the following are hyperparameters for the KerasTuner (Due to small learning rate, we should increase search_epochs)\n",
    "max_trials = 300\n",
    "executions_per_trial = 2 # The final output will be the average of this.\n",
    "patience = 8\n",
    "search_epochs = 500\n",
    "\n",
    "# the following are hyperparameters for finding the best epoch\n",
    "best_epoch_patience = 15 # We use a large patience value\n",
    "best_epoch_training_epochs = 300\n",
    "best_epoch_batch_size = 2048\n",
    "\n",
    "# We train the best model training_times, and select the best_N out of the training_times.\n",
    "Best_training_epochs = 1500\n",
    "retrain_patience = 10\n",
    "training_times = 30\n",
    "best_N = 7\n",
    "\n",
    "# the following are hyperparameters for models\n",
    "batch_size = 2048\n",
    "\n",
    "def hyper_RNN_model(hp): \n",
    "    '''We should only consider stacking RNN layers if there are bottlencks in the performance.\n",
    "    If we want to stack layers, use hp.Boolean in the block of each layer.'''\n",
    "    # note return_sequences = True only when stacking multiple RNNs, the final layer cannot have it.\n",
    "    \n",
    "    units = hp.Int(name=\"units\", min_value=64, max_value=256, step=16)\n",
    "    units2 = hp.Int(name=\"units\", min_value=32, max_value=128, step=8)\n",
    "    #units3 = hp.Int(name=\"units\", min_value=8, max_value=32, step=8)\n",
    "    DenseUnits = hp.Int(name=\"units\", min_value=16, max_value=32, step=8)\n",
    "    activation = hp.Fixed(\"activation\", \"relu\")\n",
    "    #learning_rate = hp.Fixed(\"learning_rate\", 5e-5)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"log\")\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.2, max_value=0.5, sampling=\"log\")\n",
    "    recurrent_dropout = hp.Float(name=\"recurrent_dropout\", min_value=0.1, max_value=0.3, sampling=\"log\")\n",
    "    #initializer = tf.keras.initializers.GlorotUniform(seed=42) # GlorotNormal or GlorotUniform, we add a seed\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    #model.add(Masking(mask_value=0.0, input_shape=(None, 1)))\n",
    "\n",
    "    #layers = hp.Choice(name=\"layers\", values=[\"One-RNN\", \"Two-RNN\"])\n",
    "    #recurrent_dropout = recurrent_dropout,\n",
    "    # if layers == \"One-RNN\":\n",
    "    #     model.add(SimpleRNN(units, activation = activation,  input_shape=(None, 1)))\n",
    "    # else:\n",
    "    #     model.add(SimpleRNN(units, return_sequences = True, \n",
    "    #                 activation = activation, recurrent_dropout = recurrent_dropout, input_shape=(None, 1)))\n",
    "    #     model.add(SimpleRNN(units2, activation = activation, input_shape=(None, 1)))\n",
    "\n",
    "    layers = hp.Choice(name=\"layers\", values=[\"One-RNN with Dropout\", \"One-RNN without Dropout\", \"Two-RNN with Dropout\", \"Two-RNN without Dropout\"])\n",
    "    if layers == \"One-RNN with Dropout\":\n",
    "        model.add(SimpleRNN(units, activation = activation, recurrent_dropout = recurrent_dropout, input_shape=(None, 1)))\n",
    "        if hp.Boolean(\"LayerNormalization\"):\n",
    "            model.add(LayerNormalization())\n",
    "    elif layers == \"One-RNN without Dropout\":\n",
    "        model.add(SimpleRNN(units, activation = activation, input_shape=(None, 1)))\n",
    "        if hp.Boolean(\"LayerNormalization\"):\n",
    "            model.add(LayerNormalization())\n",
    "    elif layers == \"Two-RNN with Dropout\":\n",
    "        model.add(SimpleRNN(units, return_sequences = True, \n",
    "                    activation = activation, recurrent_dropout = recurrent_dropout, input_shape=(None, 1)))\n",
    "        if hp.Boolean(\"LayerNormalization\"):\n",
    "            model.add(LayerNormalization())\n",
    "        model.add(SimpleRNN(units2, activation = activation, input_shape=(None, 1)))\n",
    "    else:\n",
    "        model.add(SimpleRNN(units, return_sequences = True, \n",
    "                    activation = activation, input_shape=(None, 1)))\n",
    "        if hp.Boolean(\"LayerNormalization\"):\n",
    "            model.add(LayerNormalization())\n",
    "        model.add(SimpleRNN(units2, activation = activation, input_shape=(None, 1)))\n",
    "\n",
    "    # layers = hp.Choice(name=\"layers\", values=[\"Two-RNN with Dropout\", \"Two-RNN without Dropout\", \"Three-RNN with Dropout\", \"Three-RNN without Dropout\"])\n",
    "    # if layers == \"Two-RNN with Dropout\":\n",
    "    #     model.add(SimpleRNN(units, return_sequences = True, \n",
    "    #                 activation = activation, recurrent_dropout = recurrent_dropout, input_shape=(None, 1)))\n",
    "    #     model.add(SimpleRNN(units2, activation = activation, input_shape=(None, 1)))\n",
    "    # elif layers == \"Two-RNN without Dropout\":\n",
    "    #     model.add(SimpleRNN(units, return_sequences = True, \n",
    "    #                 activation = activation, input_shape=(None, 1)))\n",
    "    #     model.add(SimpleRNN(units2, activation = activation, input_shape=(None, 1)))\n",
    "    # elif layers == \"Three-RNN with Dropout\":\n",
    "    #     model.add(SimpleRNN(units, return_sequences = True, \n",
    "    #                 activation = activation, recurrent_dropout = recurrent_dropout, input_shape=(None, 1)))\n",
    "    #     model.add(SimpleRNN(units2, return_sequences = True, \n",
    "    #                 activation = activation, recurrent_dropout = recurrent_dropout, input_shape=(None, 1)))\n",
    "    #     model.add(SimpleRNN(units3, activation = activation, input_shape=(None, 1)))\n",
    "    # else:\n",
    "    #     model.add(SimpleRNN(units, return_sequences = True, \n",
    "    #                 activation = activation, input_shape=(None, 1)))\n",
    "    #     model.add(SimpleRNN(units2, return_sequences = True, \n",
    "    #                 activation = activation, input_shape=(None, 1)))\n",
    "    #     model.add(SimpleRNN(units3, activation = activation, input_shape=(None, 1)))\n",
    "    if hp.Boolean(\"Dense\"):\n",
    "         model.add(Dense(DenseUnits))\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07,\n",
    "            amsgrad=hp.Boolean(\"amsgrad\"))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\",\n",
    "              metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "class DataLoader_Sequence(object):\n",
    "    def __init__(self, path):\n",
    "        df_shuffle = self.load_dataset_sequence(path)\n",
    "    \n",
    "    def load_dataset_sequence(self, path): \n",
    "        seed = 42\n",
    "        df = pd.read_csv(path, encoding='utf-8').drop(['Correct Entropy','Approx Entropy'], axis = 1)\n",
    "        df_shuffle = df.sample(frac=1., axis=0, random_state = 42).reset_index(drop=True)\n",
    "        return df_shuffle\n",
    "    \n",
    "    def split_dataset_sequence_CNN(self, df_shuffle, MIN, k, ratio): # no zero padding\n",
    "        MIN = MIN # the crucial difference with RNN, we only take the past MIN steps, this will also be the legth of vector\n",
    "        k = k     # how many datasets we want to use\n",
    "        ratio = ratio # the ratio of train-val-test split, the split is in \"timesteps\"\n",
    "        full_data_transposed = df_shuffle.T\n",
    "        \n",
    "        num_train_samples = int(ratio[0] * len(full_data_transposed))\n",
    "        num_val_samples = int(ratio[1] * len(full_data_transposed))\n",
    "        num_test_samples = len(full_data_transposed) - num_val_samples - num_train_samples\n",
    "        \n",
    "        X_train_store = {}\n",
    "        y_train_store = {}\n",
    "        for j in range(1, k+1):\n",
    "            train_data = full_data_transposed.iloc[range(0, num_train_samples), range(j-1, j)]\n",
    "            X_train = np.zeros((num_train_samples-MIN, MIN, 1)) # length is MIN\n",
    "            y_train = np.zeros((num_train_samples-MIN, 1))\n",
    "            for i in range(0, num_train_samples-MIN):\n",
    "                X_train[i, -(MIN):, :] = train_data[i:i+MIN] # the second argument means we take the last (i+MIN) values\n",
    "                y_train[i, :] = train_data[i+MIN:i+MIN+1]        \n",
    "            X_train_store[\"group\" + str(j)] = X_train\n",
    "            y_train_store[\"group\" + str(j)] = y_train\n",
    "        X_train_full = X_train_store['group' + str(1)]\n",
    "        y_train_full = y_train_store['group' + str(1)]\n",
    "        for j in range(2, k+1):\n",
    "            X_train_full = np.append(X_train_full, X_train_store['group' + str(j)], axis=0)\n",
    "            y_train_full = np.append(y_train_full, y_train_store['group' + str(j)], axis=0)\n",
    "        \n",
    "        X_val_store = {}\n",
    "        y_val_store = {}\n",
    "        for j in range(1, k+1):\n",
    "            val_data = full_data_transposed.iloc[range(num_train_samples, num_train_samples+num_val_samples), range(j-1, j)]\n",
    "            X_val = np.zeros((num_val_samples-MIN, MIN, 1))\n",
    "            y_val = np.zeros((num_val_samples-MIN, 1))\n",
    "            for i in range(0, num_val_samples-MIN):\n",
    "                X_val[i, -(MIN):, :] = val_data[i:i+MIN] \n",
    "                y_val[i, :] = val_data[i+MIN:i+MIN+1]        \n",
    "            X_val_store[\"group\" + str(j)] = X_val\n",
    "            y_val_store[\"group\" + str(j)] = y_val\n",
    "        X_val_full = X_val_store['group' + str(1)]\n",
    "        y_val_full = y_val_store['group' + str(1)]\n",
    "        for j in range(2, k+1):\n",
    "            X_val_full = np.append(X_val_full, X_val_store['group' + str(j)], axis=0)\n",
    "            y_val_full = np.append(y_val_full, y_val_store['group' + str(j)], axis=0)\n",
    "    \n",
    "        X_test_store = {}\n",
    "        y_test_store = {}\n",
    "        for j in range(1, k+1):\n",
    "            test_data = full_data_transposed.iloc[range(num_train_samples + num_val_samples, len(full_data_transposed)), range(j-1, j)]\n",
    "            X_test = np.zeros((num_test_samples-MIN, MIN, 1))\n",
    "            y_test = np.zeros((num_test_samples-MIN, 1))\n",
    "            for i in range(0, num_test_samples-MIN):\n",
    "                X_test[i, -(MIN):, :] = test_data[i:i+MIN]\n",
    "                y_test[i, :] = test_data[i+MIN:i+MIN+1]\n",
    "            X_test_store[\"group\" + str(j)] = X_test\n",
    "            y_test_store[\"group\" + str(j)] = y_test\n",
    "        X_test_full = X_test_store['group' + str(1)]\n",
    "        y_test_full = y_test_store['group' + str(1)]\n",
    "        for j in range(2, k+1):\n",
    "            X_test_full = np.append(X_test_full, X_test_store['group' + str(j)], axis=0)\n",
    "            y_test_full = np.append(y_test_full, y_test_store['group' + str(j)], axis=0)\n",
    "        return full_data_transposed, X_train_full, y_train_full, X_val_full, y_val_full, X_test_full, y_test_full\n",
    "    \n",
    "    def split_dataset_sequence_CNN_retrain(self, df_shuffle, MIN, k, ratio): # no zero padding\n",
    "        MIN = MIN # the crucial difference with RNN, we only take the past MIN steps, this will also be the legth of vector\n",
    "        k = k     # how many datasets we want to use\n",
    "        ratio = ratio # the ratio of train-val-test split, the split is in \"timesteps\"\n",
    "        full_data_transposed = df_shuffle.T\n",
    "\n",
    "        num_train_samples = int(ratio[0] * len(full_data_transposed))\n",
    "        num_val_samples = int(ratio[1] * len(full_data_transposed))\n",
    "        num_test_samples = len(full_data_transposed) - num_val_samples - num_train_samples\n",
    "\n",
    "        X_train_store = {}\n",
    "        y_train_store = {}\n",
    "        for j in range(1, k+1):\n",
    "            train_data = full_data_transposed.iloc[range(0, num_train_samples), range(j-1, j)]\n",
    "            X_train = np.zeros((num_train_samples-MIN, MIN, 1)) # length is MIN\n",
    "            y_train = np.zeros((num_train_samples-MIN, 1))\n",
    "            for i in range(0, num_train_samples-MIN):\n",
    "                X_train[i, -(MIN):, :] = train_data[i:i+MIN] # the second argument means we take the last (i+MIN) values\n",
    "                y_train[i, :] = train_data[i+MIN:i+MIN+1]        \n",
    "            X_train_store[\"group\" + str(j)] = X_train\n",
    "            y_train_store[\"group\" + str(j)] = y_train\n",
    "        X_train_full = X_train_store['group' + str(1)]\n",
    "        y_train_full = y_train_store['group' + str(1)]\n",
    "        for j in range(2, k+1):\n",
    "            X_train_full = np.append(X_train_full, X_train_store['group' + str(j)], axis=0)\n",
    "            y_train_full = np.append(y_train_full, y_train_store['group' + str(j)], axis=0)\n",
    "\n",
    "        X_test_store = {}\n",
    "        y_test_store = {}\n",
    "        for j in range(1, k+1):\n",
    "            test_data = full_data_transposed.iloc[range(num_train_samples + num_val_samples, len(full_data_transposed)), range(j-1, j)]\n",
    "            X_test = np.zeros((num_test_samples-MIN, MIN, 1))\n",
    "            y_test = np.zeros((num_test_samples-MIN, 1))\n",
    "            for i in range(0, num_test_samples-MIN):\n",
    "                X_test[i, -(MIN):, :] = test_data[i:i+MIN]\n",
    "                y_test[i, :] = test_data[i+MIN:i+MIN+1]\n",
    "            X_test_store[\"group\" + str(j)] = X_test\n",
    "            y_test_store[\"group\" + str(j)] = y_test\n",
    "        X_test_full = X_test_store['group' + str(1)]\n",
    "        y_test_full = y_test_store['group' + str(1)]\n",
    "        for j in range(2, k+1):\n",
    "            X_test_full = np.append(X_test_full, X_test_store['group' + str(j)], axis=0)\n",
    "            y_test_full = np.append(y_test_full, y_test_store['group' + str(j)], axis=0)\n",
    "        return full_data_transposed, X_train_full, y_train_full, X_test_full, y_test_full\n",
    "    \n",
    "\n",
    "\n",
    "def train_with_tuner_sequence_RNN_findepoch(path, hypermodel, save_path, model_name): \n",
    "    \n",
    "    dataloader = DataLoader_Sequence(path)\n",
    "    df_shuffle = dataloader.load_dataset_sequence(path)\n",
    "    full_data_transposed, X_train, y_train, X_val, y_val, X_test, y_test = dataloader.split_dataset_sequence_CNN(df_shuffle, MIN, k, ratio_original)\n",
    "    # note that the num_test_samples-MIN cannot be zero or below\n",
    "\n",
    "    \n",
    "\n",
    "    tuner = kt.BayesianOptimization(hypermodel, \n",
    "                                    objective=\"val_loss\", \n",
    "                                    max_trials=max_trials, \n",
    "                                    executions_per_trial=executions_per_trial, \n",
    "                                    directory=save_path, \n",
    "                                    overwrite=True, \n",
    "                                    ) \n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience),\n",
    "                 keras.callbacks.ModelCheckpoint(model_name, save_best_only=True)] \n",
    "\n",
    "    tuner.search(x = X_train, y = y_train, \n",
    "                batch_size=batch_size, epochs=search_epochs, \n",
    "                validation_data=(X_val, y_val), \n",
    "                callbacks=callbacks, verbose=1,)\n",
    "    tuner.results_summary()\n",
    "    \n",
    "    \n",
    "    # Now we find the best epoch of the model by moitoring val_loss with EarlyStopping\n",
    "    \n",
    "    best_hps = tuner.get_best_hyperparameters(max_trials)  # we pick the best set of hyperparameters\n",
    "    \n",
    "    best_model_ep = hypermodel(best_hps[0])  # we pick the best set of hyperparameters\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=best_epoch_patience)] \n",
    "    \n",
    "\n",
    "    best_model_ep.fit(x = X_train, y = y_train,\n",
    "                epochs=best_epoch_training_epochs,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=best_epoch_batch_size,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1,\n",
    "                )\n",
    "    val_loss_per_epoch = best_model_ep.history.history[\"val_loss\"]\n",
    "    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    \n",
    "    best_model_ep.summary()\n",
    "    \n",
    "    # We also plot the training and validation losses during the find best epoch process.\n",
    "    plt.plot(best_model_ep.history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(best_model_ep.history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Losses\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(best_model_ep.history.history[\"mae\"], label=\"Training MAE\")\n",
    "    plt.plot(best_model_ep.history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "    plt.title(\"Training and Validation Metrics\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return hypermodel, tuner, best_epoch, X_test, y_test, full_data_transposed\n",
    "\n",
    "\n",
    "\n",
    "def sequence_load_tuner(hypermodel, load_dir, load_project):\n",
    "    tuner = kt.BayesianOptimization(hypermodel,\n",
    "                    objective=\"val_loss\", \n",
    "                    directory=load_dir, \n",
    "                    project_name=load_project,\n",
    "                    overwrite=False,\n",
    "                    max_trials=max_trials, \n",
    "                    executions_per_trial=executions_per_trial, \n",
    "                    )\n",
    "    return tuner\n",
    "\n",
    "\n",
    "def sequence_load_best_hyperparam(hypermodel, load_dir, load_project, top_n):\n",
    "    tuner = sequence_load_tuner(hypermodel, load_dir, load_project)\n",
    "    return tuner.get_best_hyperparameters(top_n)\n",
    "\n",
    "  # if you want to see the top_n hyper params, use below\n",
    "  # best_trials = tuner.oracle.get_best_trials(num_trials=top_n)\n",
    "  # for trial in best_trials:\n",
    "    # trial.summary()\n",
    "    # model = tuner.load_model(trial)\n",
    "    # Do some stuff to the model\n",
    "\n",
    "\n",
    "\n",
    "def sequence_train_with_tuner_ensemble_models(path, hypermodel, hp_dir, hp_proj): \n",
    "    # the saved checkpoint is under \"hp_dir/hp_proj/\"\n",
    "\n",
    "    dataloader = DataLoader_Sequence(path)\n",
    "    df_shuffle = dataloader.load_dataset_sequence(path)\n",
    "    full_data_transposed, X_train, y_train, X_val, y_val, X_test, y_test = dataloader.split_dataset_sequence_CNN(df_shuffle, MIN, k, ratio_original)\n",
    "    # note that the num_test_samples-MIN cannot be zero or below\n",
    "    \n",
    "    tuner = sequence_load_tuner(hypermodel, hp_dir, hp_proj)\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience)]\n",
    "        \n",
    "    #  keras.callbacks.ModelCheckpoint(model_name, save_best_only=True)] \n",
    "    tuner.search(x = X_train, y = y_train, \n",
    "                batch_size=batch_size, epochs=search_epochs, \n",
    "                validation_data=(X_val, y_val), \n",
    "                callbacks=callbacks, verbose=1,)\n",
    "    tuner.results_summary()\n",
    "    return tuner\n",
    "\n",
    "\n",
    "\n",
    "#def sequence_model_retrain(path, hypermodel, tuner, best_epoch, retrain_patience, k, best_hps):\n",
    "def sequence_model_retrain(path, hypermodel, retrain_patience, k, best_hps, save_model_path):\n",
    "    # We take the best model found and retrain with the full training data.\n",
    "    # We train multiple times and take the average of the best N models by looking at the overall relative errors.\n",
    "    # Note that the Best_training_epochs will only be the maximum possible epoch, we will monitor by EarlyStopping with \"loss\".\n",
    "    # We will only plot the training losses of the best_N models.\n",
    "    \n",
    "    dataloader = DataLoader_Sequence(path)\n",
    "    df_shuffle = dataloader.load_dataset_sequence(path)\n",
    "    full_data_transposed_old, X_train_old, y_train_old, X_val_old, y_val_old, X_test_old, y_test_old = dataloader.split_dataset_sequence_CNN(df_shuffle, MIN, k, ratio_original)\n",
    "    full_data_transposed, X_train, y_train, X_test, y_test = dataloader.split_dataset_sequence_CNN_retrain(df_shuffle, MIN, k, ratio)\n",
    "    # note that the num_test_samples-MIN cannot be zero or below\n",
    "    \n",
    "    final_test = pd.DataFrame(y_test)\n",
    "    final_test.columns = ['Targets'] # The final targets\n",
    "    \n",
    "    ratio_retrain = len(X_train)/len(X_train_old) \n",
    "    \n",
    "    #Best_training_epochs = int(best_epoch * ratio_retrain)\n",
    "    \n",
    "    model_retrain = {}\n",
    "    model_history_retrain = {}\n",
    "    df_store_test = {}\n",
    "    \n",
    "    #best_hps = tuner.get_best_hyperparameters(max_trials)\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"loss\", patience=retrain_patience, restore_best_weights=True)] \n",
    "    \n",
    "    trained_models = {}\n",
    "    j = 0\n",
    "    while j < training_times:\n",
    "        best_model_retrain = hypermodel(best_hps[0]) # We take again the best set of hyperparameters.\n",
    "        best_model_retrain.fit(X_train, y_train, \n",
    "                      epochs=Best_training_epochs,\n",
    "                      batch_size = batch_size,\n",
    "                      callbacks=callbacks,\n",
    "                      verbose=1,\n",
    "                      )\n",
    "\n",
    "        model_retrain[str(j)] = best_model_retrain\n",
    "        model_history_retrain[str(j)] = best_model_retrain.history.history\n",
    "        \n",
    "        best_model_retrain.save(os.path.join(save_model_path, f\"model_{j}.h5\"))\n",
    "        trained_models[j] = f\"model_{j}.h5\"\n",
    "\n",
    "\n",
    "        test_predictions = best_model_retrain.predict(X_test) # for the original train-test split\n",
    "        test_pred = pd.DataFrame(test_predictions)\n",
    "        test_pred.columns = ['Model # '+ str(j) + ' Predictions']\n",
    "        df_store_test[str(j)] = test_pred\n",
    "\n",
    "        j = j + 1\n",
    "        \n",
    "\n",
    "    return trained_models, model_retrain, model_history_retrain, df_store_test, final_test\n",
    "    \n",
    "def sequence_model_average(trained_models, df_store_test, final_test, save_path):\n",
    "    \n",
    "    # We choose the best_N models by monitoring the overall relative errors \n",
    "    df_test_all = df_store_test[str(0)]\n",
    "    for k in range(1, training_times):\n",
    "        df_test_all = pd.concat([df_test_all, df_store_test[str(k)]], axis=1)\n",
    "\n",
    "    df_compare = pd.concat([final_test, df_test_all], axis = 1)\n",
    "    print(df_compare)\n",
    "    for l in range(0, training_times):\n",
    "        df_rel_error = (abs(df_compare['Targets']-df_compare['Model # '+ str(l) + ' Predictions'])/df_compare['Targets'])*100\n",
    "        df_compare.insert(2+2*l, 'Rel Error (%) ' + str(l), df_rel_error)\n",
    "    print(df_compare)\n",
    "    \n",
    "    column_sums = df_compare[['Rel Error (%) '+ str(m) for m in range(0, training_times)]].sum()\n",
    "    smallest_columns = column_sums.nsmallest(best_N)\n",
    "    \n",
    "    print(column_sums)\n",
    "    print(f\"{smallest_columns.index} have the smallest relative errors.\")\n",
    "    \n",
    "    column_indices = df_compare.columns.get_indexer(smallest_columns.index)\n",
    "\n",
    "    # Select the columns one before each of these columns\n",
    "    previous_columns = df_compare.iloc[:, column_indices - 1]\n",
    "    \n",
    "    # Find the model numbers\n",
    "    column_names = list(previous_columns.columns)\n",
    "    pattern = r'\\d+'\n",
    "    model_numbers = [int(re.search(pattern, column_name).group()) for column_name in column_names]\n",
    "    \n",
    "    mean_values = previous_columns.mean(axis=1)\n",
    "    mean_values = pd.DataFrame(mean_values)\n",
    "    mean_values.columns=['Model Average']\n",
    "    df_compare_final = pd.concat([final_test, mean_values], axis=1)\n",
    "    \n",
    "    df_abs_error = abs(df_compare_final['Targets']-df_compare_final['Model Average'])\n",
    "    df_rel_error = (abs(df_compare_final['Targets']-df_compare_final['Model Average'])/df_compare_final['Targets'])*100  \n",
    "    \n",
    "    df_compare_final.insert(2, 'Absolute Errors', df_abs_error)\n",
    "    df_compare_final.insert(3, 'Relative Errors (%)', df_rel_error)\n",
    "    \n",
    "    print(df_compare_final) # The final predictions on the test data.\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(df_rel_error, '.')\n",
    "    plt.title(\"Relative Errors (%) for Model Predictions\")\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.distplot(df_compare_final['Relative Errors (%)'])\n",
    "#     sns.kdeplot(df_compare_final['Relative Errors (%)'], ax=ax, shade=True, color='blue', alpha=0.05)\n",
    "\n",
    "\n",
    "    ax.set_title('Density Plot of Relative Errors for Test Data')\n",
    "    ax.set_xlabel('Relative Errors (%)')\n",
    "    ax.set_ylabel('Density')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'TestDensity.jpg'))\n",
    "    return df_compare, df_compare_final, model_numbers\n",
    "\n",
    "\n",
    "def sequence_model_plots(model_history_retrain, model_numbers, save_path):\n",
    "    # We only plot the losses of the best_N models, and will rename them in ascending order.\n",
    "    \n",
    "    x_range_list = []    \n",
    "    legend_labels = []\n",
    "    #fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=False, figsize=(20, 16))\n",
    "    #fig, (ax1, ax3) = plt.subplots(2, 1, sharex=False, figsize=(20, 16))\n",
    "    fig, ax1 = plt.subplots(1, 1, sharex=False, figsize=(20, 16))\n",
    "    y_max = []\n",
    "    y_max_mae = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(model_numbers):\n",
    "        number = model_numbers[i]\n",
    "        y_loss = model_history_retrain[str(number)]['loss']\n",
    "        y_metrics = model_history_retrain[str(number)][\"mae\"]\n",
    "        x = range(1,len(y_loss)+1)\n",
    "        \n",
    "        y_max.append(max(model_history_retrain[str(number)][\"loss\"][9:].copy()))\n",
    "        y_max_mae.append(max(model_history_retrain[str(number)][\"mae\"][9:].copy()))\n",
    "\n",
    "        ax1.plot(x, y_loss, '--')\n",
    "        #ax2.plot(x, y_loss, '.--')\n",
    "\n",
    "        #ax3.plot(x, y_metrics, '--')\n",
    "        #ax4.plot(x, y_metrics, '.--')\n",
    "\n",
    "        x_range_list.append(len(y_loss))\n",
    "        \n",
    "        legend_labels.append('Model #' + str(i) + ' (Epoch= '+ str(x_range_list[i])+ ')')\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    fig.legend(legend_labels, loc=\"right\", bbox_to_anchor=(1.16, 0.5), frameon=True, edgecolor='black', borderpad=1, labelspacing=2, handlelength=3) \n",
    "    ax1.set_title(\"Loss Function\",fontsize=40)\n",
    "    ax1.set_xlabel(\"Epoch\", fontsize=25)\n",
    "    ax1.set_ylabel(\"MSE\", fontsize=25)\n",
    "    ax1.set_xlim([1, max(x_range_list)])\n",
    "    ax1.set_xticks(range(50,max(x_range_list),50))\n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "#     ax2.set_title(\"Loss Function-truncated view\")\n",
    "#     ax2.set_xlabel(\"Epoch\")\n",
    "#     ax2.set_ylabel(\"MSE\")\n",
    "#     ax2.set_xlim([10, max(x_range_list)])\n",
    "#     ax2.set_xticks(range(10,max(x_range_list),5))\n",
    "#     ax2.set_ylim([0.0, 1.2*max(y_max)]) \n",
    "#     ax2.set_yscale('log')\n",
    "\n",
    "#     ax3.set_title(\"Metric\")\n",
    "#     ax3.set_xlabel(\"Epoch\")\n",
    "#     ax3.set_ylabel(\"MAE\")\n",
    "#     ax3.set_xlim([1, max(x_range_list)])\n",
    "#     ax3.set_xticks(range(5,max(x_range_list),5))\n",
    "#     ax3.set_yscale('log')\n",
    "\n",
    "#     ax4.set_title(\"Metrics-truncated view\")\n",
    "#     ax4.set_xlabel(\"Epoch\")\n",
    "#     ax4.set_ylabel(\"MAE\")\n",
    "#     ax4.set_xlim([10, max(x_range_list)])\n",
    "#     ax4.set_xticks(range(10,max(x_range_list),5))\n",
    "#     ax4.set_ylim([0.0, 1.2*max(y_max_mae)]) \n",
    "#     ax4.set_yscale('log')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'LossFunction.jpg'))\n",
    "    \n",
    "def prediction_unseen_data(trained_models, model_numbers, path, save_path): \n",
    "    # for predictions of the unseen data.\n",
    "    # We start with the MIN steps of data, predict the MIN+1 step, but \"not\" reusing the prediction for next prediction. \n",
    "    dataloader = DataLoader_Sequence(path)\n",
    "    df_shuffle = dataloader.load_dataset_sequence(path)\n",
    "    full_data_transposed, X_train, y_train, X_val, y_val, X_test, y_test = dataloader.split_dataset_sequence_CNN(df_shuffle, MIN, k, ratio_original)\n",
    "\n",
    "    num_train_samples = int(len(full_data_transposed))\n",
    "\n",
    "    X_train_store = {}\n",
    "    y_train_store = {}\n",
    "    for j in range(k+1, len(full_data_transposed.columns)+1):\n",
    "        train_data = full_data_transposed.iloc[range(0, num_train_samples), range(j-1, j)]\n",
    "        X_train = np.zeros((num_train_samples-MIN, MIN, 1)) # length is MIN\n",
    "        y_train = np.zeros((num_train_samples-MIN, 1))\n",
    "        for i in range(0, num_train_samples-MIN):\n",
    "            X_train[i, -(MIN):, :] = train_data[i:i+MIN] # the second argument means we take the last (i+MIN) values\n",
    "            y_train[i, :] = train_data[i+MIN:i+MIN+1]        \n",
    "        X_train_store[\"group\" + str(j)] = X_train\n",
    "        y_train_store[\"group\" + str(j)] = y_train\n",
    "    X_train_full = X_train_store['group' + str(k+1)]\n",
    "    y_train_full = y_train_store['group' + str(k+1)]\n",
    "    for j in range(k+2, len(full_data_transposed.columns)+1):\n",
    "        X_train_full = np.append(X_train_full, X_train_store['group' + str(j)], axis=0)\n",
    "        y_train_full = np.append(y_train_full, y_train_store['group' + str(j)], axis=0)\n",
    "\n",
    "    predictions_set = {}\n",
    "    j = 0\n",
    "    while j < len(model_numbers):\n",
    "        number = model_numbers[j]\n",
    "        model = tf.keras.models.load_model(trained_models[number])\n",
    "\n",
    "        predictions = model.predict(X_train_full)\n",
    "        pred = pd.DataFrame(predictions)\n",
    "        \n",
    "        predictions_set[str(j)] = pred\n",
    "        j = j + 1\n",
    "    \n",
    "    df_predictions = predictions_set[str(0)]\n",
    "    for l in range(1, len(model_numbers)):\n",
    "        pred = pd.concat([df_predictions, predictions_set[str(l)]], axis=1)\n",
    "        \n",
    "    pred = pred.mean(axis=1) \n",
    "    pred = pd.DataFrame(pred)\n",
    "    pred.columns = ['Model Predictions']\n",
    "    test = pd.DataFrame(y_train_full)\n",
    "    test.columns = ['Targets']\n",
    "    df_compare = pd.concat([test, pred], axis = 1)\n",
    "    print(df_compare)\n",
    "    \n",
    "    df_abs_error = abs(df_compare['Targets']-df_compare['Model Predictions'])\n",
    "    df_rel_error = (abs(df_compare['Targets']-df_compare['Model Predictions'])/df_compare['Targets'])*100  \n",
    "    df_compare1 = pd.concat([df_abs_error, df_rel_error], axis = 1)\n",
    "    df_compare1.columns = ['Abs Error for Model', 'Rel Error for Model']\n",
    "    print(df_compare1)\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(df_rel_error, '.')\n",
    "    plt.title(\"Relative Errors for Model Predictions\")\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#     sns.kdeplot(df_compare_final['Relative Errors (%)'], ax=ax, shade=True, color='blue', alpha=0.05)\n",
    "    sns.distplot(df_compare1['Rel Error for Model'])\n",
    "\n",
    "\n",
    "    ax.set_title('Density Plot of Relative Errors for Unseen Data')\n",
    "    ax.set_xlabel('Relative Errors (%)')\n",
    "    ax.set_ylabel('Density')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'UnseenDensity.jpg'))\n",
    "    \n",
    "    return df_compare, df_compare1\n",
    "\n",
    "\n",
    "def prediction_sequence(trained_models, model_numbers, path, n): # predict the next n values \n",
    "    # This will always be using the past MIN data to predict the next value.\n",
    "    dataloader = DataLoader_Sequence(path)\n",
    "    df_shuffle = dataloader.load_dataset_sequence(path)\n",
    "    full_data_transposed, X_train, y_train, X_val, y_val, X_test, y_test = dataloader.split_dataset_sequence_CNN(df_shuffle, MIN, k, ratio_original)\n",
    "    \n",
    "    sequence_test = np.zeros((k, MIN, 1))\n",
    "    for m in range(1, k+1):\n",
    "        sequence_update = full_data_transposed.iloc[-(MIN):, range(m-1, m)]\n",
    "        sequence_test[m-1, -(MIN):, :] = sequence_update[0:MIN]\n",
    "    \n",
    "\n",
    "    \n",
    "    predictions_set = {}\n",
    "    j = 0\n",
    "    while j < len(model_numbers):\n",
    "        number = model_numbers[j]\n",
    "        model = tf.keras.models.load_model(trained_models[number])\n",
    "\n",
    "        predictions = model.predict(sequence_test)\n",
    "        pred = pd.DataFrame(predictions)\n",
    "        \n",
    "        predictions_set[str(j)] = pred\n",
    "        j = j + 1\n",
    "    \n",
    "    df_predictions = predictions_set[str(0)]\n",
    "    for l in range(1, len(model_numbers)):\n",
    "        pred = pd.concat([df_predictions, predictions_set[str(l)]], axis=1)\n",
    "        \n",
    "    pred = pred.mean(axis=1) \n",
    "    pred = pd.DataFrame(pred)\n",
    "    pred.columns = [len(full_data_transposed)+1]\n",
    "    pred = pred.T\n",
    "    sequence_update = pd.concat([full_data_transposed.iloc[range(0, len(full_data_transposed)), range(0, k)], pred], axis = 0)\n",
    "    \n",
    "    i = 2\n",
    "    while i <= n:\n",
    "        sequence_test1 = np.zeros((k, MIN, 1))\n",
    "        for j in range(1, k+1):\n",
    "            sequence_update1 = sequence_update.iloc[-(MIN):, range(j-1, j)]\n",
    "            sequence_test1[j-1, -(MIN):, :] = sequence_update1[0:MIN]\n",
    "        \n",
    "        predictions_set = {}\n",
    "        j = 0\n",
    "        while j < len(model_numbers):\n",
    "            number = model_numbers[j]\n",
    "            model = tf.keras.models.load_model(trained_models[number])\n",
    "\n",
    "            predictions = model.predict(sequence_test1)\n",
    "            pred = pd.DataFrame(predictions)\n",
    "\n",
    "            predictions_set[str(j)] = pred\n",
    "            j = j + 1\n",
    "\n",
    "        df_predictions = predictions_set[str(0)]\n",
    "        for l in range(1, len(model_numbers)):\n",
    "            pred = pd.concat([df_predictions, predictions_set[str(l)]], axis=1)\n",
    "\n",
    "        pred = pred.mean(axis=1) \n",
    "        pred = pd.DataFrame(pred)\n",
    "        pred.columns = [len(full_data_transposed)+i]\n",
    "        pred = pred.T\n",
    "        sequence_update = pd.concat([sequence_update, pred], axis = 0)\n",
    "            \n",
    "        i = i + 1\n",
    "    return sequence_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42953ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN = 5 # the length of each vector\n",
    "k = 1000 # how many datasets we want to use out of the full 10000 sets\n",
    "ratio_original = (0.6, 0.2, 0.2)\n",
    "\n",
    "# load the datasets\n",
    "dataloader = DataLoader_Sequence('')\n",
    "df_shuffle = dataloader.load_dataset_sequence('')\n",
    "full_data_transposed, X_train, y_train, X_val, y_val, X_test, y_test = dataloader.split_dataset_sequence_CNN(df_shuffle, MIN, k, ratio_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0645200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = ''\n",
    "save_hp_dir = ''\n",
    "save_hp_proj = ''\n",
    "save_model_path = ''\n",
    "save_fig_dir = ''\n",
    "\n",
    "# this is to assure the path name is correct\n",
    "import os\n",
    "assert(os.path.exists(train_data_path))\n",
    "assert(os.path.exists(test_data_path))\n",
    "assert(os.path.exists(save_hp_dir))\n",
    "assert(os.path.exists(save_model_path))\n",
    "assert(os.path.exists(save_fig_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One could run the following line by line for each example considered in section 4 of arXiv:2305.00997\n",
    "\n",
    "tuner = sequence_train_with_tuner_ensemble_models(train_data_path, hyper_RNN_model, save_hp_dir, save_hp_proj)\n",
    "\n",
    "best_hps = sequence_load_best_hyperparam(hyper_RNN_model, save_hp_dir, save_hp_proj, max_trials)\n",
    "trained_models, model_retrain, model_history_retrain, df_store_test, final_test = sequence_model_retrain(train_data_path, hyper_RNN_model, retrain_patience, k, best_hps, save_model_path)\n",
    "\n",
    "best_N = 2\n",
    "df_compare, df_compare_final, model_numbers = sequence_model_average(trained_models, df_store_test, final_test, save_fig_dir)\n",
    "\n",
    "sequence_model_plots(model_history_retrain, model_numbers, save_fig_dir)\n",
    "\n",
    "df_compare, df_compare1 = prediction_unseen_data(trained_models, model_numbers, train_data_path, save_fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12032fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating figures for the relative errors of test data\n",
    "\n",
    "fig_dist, ax3 = plt.subplots(1,1,figsize=(10,5))\n",
    "sns.distplot(df_compare_final['Relative Errors (%)'])\n",
    "ax3.set_title('Density Plot of Relative Errors for the p Test Data')\n",
    "ax3.set_xlabel('Relative Errors (%)')\n",
    "ax3.set_xlim(left=-1, right=10)\n",
    "ax3.set_ylabel('Density')\n",
    "plt.savefig(save_fig_dir+\"/relative_error_density1.jpg\")\n",
    "\n",
    "fig_dist, ax3 = plt.subplots(1,1,figsize=(10,5))\n",
    "# sns.distplot(df_compare_final['Relative Errors (%)'])\n",
    "sns.distplot(df_compare1['Rel Error for Model'], color=\"green\")\n",
    "ax3.set_title('Density Plot of Relative Errors for the q Test Data')\n",
    "ax3.set_xlabel('Relative Errors (%)')\n",
    "ax3.set_xlim(left=-2, right=25)\n",
    "ax3.set_ylabel('Density')\n",
    "plt.savefig(save_fig_dir+\"/relative_error_density3.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debcea8",
   "metadata": {},
   "source": [
    "### The following provides the source code for reproducing results in section 5 of arXiv: 2305.00997 of studying the expressivity of von Neumann entropy using the Fourier series representation of the generating function. This is based on PennyLane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f225b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# plt.style.reload_library()\n",
    "# plt.style.use(['science','no-latex'])\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def square_loss(targets, predictions):\n",
    "    loss = 0\n",
    "    for t, p in zip(targets, predictions):\n",
    "        loss += (t - p) ** 2\n",
    "    loss = loss / len(targets)\n",
    "    return 0.5*loss\n",
    "\n",
    "def target_Fourier_series(x): # generate the target Fourier series with input above\n",
    "    series = coeff0 # initialize the series\n",
    "    for order, coeff in order_coeffs:\n",
    "        exponent = np.complex128(scaling * order * x * 1j)\n",
    "        conj_coeff = np.conjugate(coeff)\n",
    "        series += coeff * np.exp(exponent) + conj_coeff * np.exp(-exponent)\n",
    "    return np.real(series)\n",
    "\n",
    "def target_Fourier_series_nonReal(x): \n",
    "  # generate the target Fourier series for the case of non-real function\n",
    "  # note also that we need to use 2pi for each order corresponding to the Fourier parameter\n",
    "    series = coeff0 # initialize the series\n",
    "    for order, coeff in order_coeffs:\n",
    "        exponent = np.complex128(scaling * 2 * np.pi * order * x * 1j)\n",
    "        series += coeff * np.exp(exponent)\n",
    "    return np.real(series)\n",
    "\n",
    "def target_Fourier_series_stretched(x): # For stretched interval \n",
    "    series = coeff0 # initialize the series\n",
    "    L = 1\n",
    "    for order, coeff in order_coeffs:\n",
    "        exponent = np.complex128(scaling * 2 * np.pi * order * x * 1j/L)\n",
    "        conj_coeff = np.conjugate(coeff)\n",
    "        series += coeff * np.exp(exponent)/np.sqrt(L) + conj_coeff * np.exp(-exponent)/np.sqrt(L)\n",
    "    return np.real(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the generating function as the Fourier series\n",
    "\n",
    "# degree = 1  # degree of the target function \n",
    "scaling = 1  # scaling of the data\n",
    "#order_coeffs = [(1, -0.196497), (2, -0.0752353), (3, -0.0592326), (4, -0.0395808)] # coefficients of non-zero frequencies, c1, c2,..etc.\n",
    "\n",
    "# We choose the parameters for the coefficients to be L=2, epsilon=0.1\n",
    "\n",
    "order_coeffs = [(1, 0.028467 + 0.131412j), (2, 0.010255 + 0.070899j), (3, 0.005445 + 0.048685j), (4, 0.003413 + 0.037097j)] # coefficients of non-zero frequencies, c1, c2,..etc. # For the generating function\n",
    "coeff0 = 0.35978  # coefficient of zero frequency\n",
    "\n",
    "# Note that we need to change the interval in accordance with the stretched case\n",
    "x = np.linspace(0, 1, 300, requires_grad=False) # requires_grad=False where the parameters are not considered as trainable\n",
    "\n",
    "# x1 = np.linspace(0,0.2,50,  requires_grad=False)\n",
    "# x2 = np.linspace(0.2, 0.8, 100, requires_grad=False)\n",
    "# x3 = np.linspace(0.8, 1.0, 50, requires_grad=False)\n",
    "# x = np.concatenate((x1, x2, x3))\n",
    "target_y = np.array([target_Fourier_series_stretched(x_) for x_ in x], requires_grad=False)\n",
    "\n",
    "#plt.plot(x, target_y, c='black') #, c='red'\n",
    "plt.scatter(x, target_y, linewidths=1 , facecolor='white', edgecolor='red', marker='.', alpha=1.0) #, facecolor='white', edgecolor='black'\n",
    "plt.ylim(np.amin(target_y)-0.1, np.amax(target_y)+0.1)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serial Model\n",
    "\n",
    "dev = qml.device('default.qubit', wires=1)\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Data-encoding circuit block.\"\"\"\n",
    "    qml.RX(scaling * x, wires=0)\n",
    "\n",
    "def W(theta):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    qml.Rot(theta[0], theta[1], theta[2], wires=0)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def serial_quantum_model(weights, x):\n",
    "\n",
    "    for theta in weights[:-1]:\n",
    "        W(theta)\n",
    "        S(x)\n",
    "\n",
    "    # (L+1)'th unitary\n",
    "    W(weights[-1])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0))\n",
    "    #PauliZ meansure will always restrict the y-range to be [-1,1] since the eigenvalues are [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can run the following multiple times, each time sampling different weights, and therefore different quantum models.\n",
    "\n",
    "r = 6 # number of times the encoding gets repeated (here equal to the number of layers) \n",
    "# this value should be the same as the order of Fourier series\n",
    "weights = 2 * np.pi * np.random.random(size=(r+1, 3), requires_grad=True) # some random initial weights\n",
    "scaling_model = 6\n",
    "\n",
    "# Note that we need to change the interval and the number of samples below in accordance with the target\n",
    "x = np.linspace(0, scaling_model, 300, requires_grad=False)\n",
    "\n",
    "# x1 = np.linspace(0, 0.2,50,  requires_grad=False)\n",
    "# x2 = np.linspace(0.2, 0.8, 100, requires_grad=False)\n",
    "# x3 = np.linspace(0.8, 1.0, 50, requires_grad=False)\n",
    "# x_con = np.concatenate((x1, x2, x3))\n",
    "# x = scaling_model * ((x_con - x_con.min()) / (x_con.max() - x_con.min()))\n",
    "random_quantum_model_y = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(x, random_quantum_model_y, c='black')\n",
    "ax.set_ylim(np.amin(random_quantum_model_y)-0.1, np.amax(random_quantum_model_y)+0.1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "print(qml.draw(serial_quantum_model)(weights, x[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, x, y):\n",
    "    predictions = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "    return square_loss(y, predictions)\n",
    "\n",
    "max_steps = 1500\n",
    "opt = qml.AdamOptimizer(0.005)\n",
    "batch_size = 100\n",
    "cst = [cost(weights, x, target_y)]  # initial cost\n",
    "best_weights = weights\n",
    "best_cost = cst[0]\n",
    "best_step = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Select batch of data\n",
    "    batch_index = np.random.randint(0, len(x), (batch_size,))\n",
    "    x_batch = x[batch_index]\n",
    "    y_batch = target_y[batch_index]\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    weights, _, _ = opt.step(cost, weights, x_batch, y_batch)\n",
    "\n",
    "    # Save, and possibly print, the current cost\n",
    "    c = cost(weights, x, target_y)\n",
    "    cst.append(c)\n",
    "    if (step + 1) % 1 == 0:\n",
    "        print(\"Cost at step {0:3}: {1}\".format(step + 1, c))\n",
    "    if c < best_cost:\n",
    "        best_weights = weights\n",
    "        best_cost = c\n",
    "        best_step = step + 1\n",
    "\n",
    "#print(\"Best weights: \", best_weights)\n",
    "print(\"Best cost: \", best_cost)\n",
    "print(\"Found at step: \", best_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1735a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(range(len(cst)), cst, '--', color=\"red\")\n",
    "ax.set_xlim(0,983)\n",
    "# ax.set_ylim(0, np.amax(cst)*1.1)\n",
    "ax.set_title('Loss Function')\n",
    "ax.set_xlabel('Epochs')\n",
    "xticks = list(range(200,best_step,200))\n",
    "xticks.append(best_step)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13201cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [serial_quantum_model(best_weights, x_) for x_ in x]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(x, target_y, c='black')\n",
    "ax.scatter(x, target_y, facecolor='white', edgecolor='black')\n",
    "ax.plot(x, predictions, c='red')\n",
    "ax.set_ylim(np.amin(target_y)-0.1, np.amax(target_y)+0.1)\n",
    "#ax.set_title('Density Plot of Relative Errors for Test Data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "fig.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9849ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Model\n",
    "\n",
    "from pennylane.templates import StronglyEntanglingLayers\n",
    "\n",
    "# scaling = 1\n",
    "r = 5\n",
    "\n",
    "dev = qml.device('default.qubit', wires=r)\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Data-encoding circuit block.\"\"\"\n",
    "    for w in range(r):\n",
    "        qml.RX(scaling * x, wires=w)\n",
    "\n",
    "def W(theta):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    StronglyEntanglingLayers(theta, wires=range(r))\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def parallel_quantum_model(weights, x):\n",
    "\n",
    "    W(weights[0])\n",
    "    S(x)\n",
    "    W(weights[1])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_block_layers = 3\n",
    "weights = 2 * np.pi * np.random.random(size=(2, trainable_block_layers, r, 3), requires_grad=True)\n",
    "model_scaling = 6\n",
    "\n",
    "x = np.linspace(0, model_scaling, 300, requires_grad=False)\n",
    "random_quantum_model_y = [parallel_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(x, random_quantum_model_y, c='black')\n",
    "ax.set_ylim(np.amin(random_quantum_model_y)-0.1, np.amax(random_quantum_model_y)+0.1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "print(qml.draw(parallel_quantum_model)(weights, x[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016798a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, x, y):\n",
    "    predictions = [parallel_quantum_model(weights, x_) for x_ in x]\n",
    "    return square_loss(y, predictions)\n",
    "\n",
    "max_steps = 1000\n",
    "opt = qml.AdamOptimizer(0.005)\n",
    "batch_size = 100\n",
    "cst = [cost(weights, x, target_y)]  # initial cost\n",
    "best_weights = weights\n",
    "best_cost = cst[0]\n",
    "best_step = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Select batch of data\n",
    "    batch_index = np.random.randint(0, len(x), (batch_size,))\n",
    "    x_batch = x[batch_index]\n",
    "    y_batch = target_y[batch_index]\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    weights, _, _ = opt.step(cost, weights, x_batch, y_batch)\n",
    "\n",
    "    # Save, and possibly print, the current cost\n",
    "    c = cost(weights, x, target_y)\n",
    "    cst.append(c)\n",
    "    if (step + 1) % 1 == 0:\n",
    "        print(\"Cost at step {0:3}: {1}\".format(step + 1, c))\n",
    "    if c < best_cost:\n",
    "        best_weights = weights\n",
    "        best_cost = c\n",
    "        best_step = step + 1\n",
    "\n",
    "#print(\"Best weights: \", best_weights)\n",
    "print(\"Best cost: \", best_cost)\n",
    "print(\"Found at step: \", best_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a36756",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(range(len(cst)), cst, '--', color=\"red\")\n",
    "ax.set_xlim(0,917)\n",
    "# ax.set_ylim(0, np.amax(cst)*1.1)\n",
    "ax.set_title('Loss Function')\n",
    "ax.set_xlabel('Epochs')\n",
    "xticks = list(range(200,best_step,200))\n",
    "xticks.append(best_step)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [parallel_quantum_model(best_weights, x_) for x_ in x]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(x, target_y, c='black')\n",
    "ax.scatter(x, target_y, facecolor='white', edgecolor='black')\n",
    "ax.plot(x, predictions, c='red')\n",
    "ax.set_ylim(np.amin(target_y)-0.1, np.amax(target_y)+0.1)\n",
    "#ax.set_title('Density Plot of Relative Errors for Test Data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "fig.tight_layout()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
